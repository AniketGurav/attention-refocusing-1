First, install [detectron2](https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md).
Then, download the [UniDet](https://github.com/xingyizhou/UniDet)'s
[weights](https://drive.google.com/file/d/110JSpmfNU__7T3IMSJwv0QSfLLo_AqtZ)
and [configurations](https://github.com/xingyizhou/UniDet/blob/master/configs/Partitioned_COI_RS101_2x.yaml).

## Counting, Spatail, Size Composition

Run the
[inference code](detection/UniDet-master/demo.py)
to generate the bounding boxes and save them as follows:
```bash
cd data_evaluate_LLM/eval_metrics/detection/UniDet-master
```
```python
python demo.py --config-file configs/Partitioned_COI_RS101_2x.yaml \
--input [path_to_images]/* --pkl_pth [path_save] \
--output [detected_images] --opts MODEL.WEIGHTS [model_weight]
```
Where:
```

1) path_to_images: folder of images need to evaluate
2) path_save: the path to the output pkl file to save detected information
3) detected_images: path to the folder to save visual detected images
4) model_weight: path to the weights downloaded above
```
eg.
```python demo.py --config-file configs/Partitioned_COI_RS101_2x.yaml --input  
"GLIGEN/visual/counting_500_img/*" --pkl_pth "../../counting/counting_500_1499.pkl"  --output "detected" --opts MODEL.WEIGHTS "Partitioned_COI_RS101_2x.pth"
```


### Counting 
Run the 
[calc_counting_acc.py](counting/calc_counting_acc.py)
to calculate the counting accuracy, as follows:
```bash
cd data_evaluate_LLM/eval_metrics/counting
```

```python
python calc_counting_acc.py [Input pkl path] [GT-csv] [Number of Iteration]
```
eg.
```python
python calc_counting_acc.py 'Attend-and-Excite/counting_5000.pkl,Attend-and-Excite/counting_500_1499.pkl' ../../HRS/counting_prompts.csv 1
```

### Spatial composition
Run the 
[calc_spatial_relation_acc.py](compositions/calc_spatial_relation_acc.py)
to calculate the spatial composition accuracy, as follows:
```bash
cd data_evaluate_LLM/eval_metrics/compositions
```
```python
python calc_spatial_relation_acc.py [Input pkl path] [GT-csv] [Number of Iteration]
```
eg.
```python
python calc_spatial_relation_acc.py 'Attend-and-Excite/spatial_500.pkl,Attend-and-Excite/spatial_1000.pkl' ../../HRS/spatial_compositions_prompts.csv 1
```
### Size
Run the 
[calc_size_comp_acc.py](compositions/calc_size_comp_acc.py)
to calculate the size composition accuracy, as follows:
```bash
cd data_evaluate_LLM/eval_metrics/compositions
```
```python
python calc_size_comp_acc.py [Input pkl path] [GT-csv] [Number of Iteration]
```
eg.
```python
python calc_spatial_relation_acc.py 'Attend-and-Excite/size.pkl' ../../HRS/size_compositions_prompts.csv 1
```
Where:
```
1) Input pkl path: is the output file from runing UniDet as shown in the above steps, if there are multi pkl files for one category, use "," to seperate each path file. eg:'counting_500.pkl,counting_1000.pkl'
2) GT-csv: is the csv file which can be generated by running the prompt generation code or downloaded directly from our published prompts.
3) Number of Iteration: is integer number stand for number of runs, then we take the average of their scores. (should be between 1-3)
```
## Color Composition:
We adopt [MaskDINO](https://arxiv.org/pdf/2206.02777.pdf) [CVPr 2023] for the instance segmentation.
Download the [model weight](https://github.com/IDEA-Research/detrex-storage/releases/download/maskdino-v0.1.0/maskdino_swinl_50ep_300q_hid2048_3sd1_panoptic_58.3pq.pth), the corresponding [config](colors/MaskDINO/configs/coco/instance-segmentation/swin/maskdino_R50_bs16_50ep_4s_dowsample1_2048.yaml) 
First, run the 
[inference code](colors/MaskDINO/demo/demo.py)
to predict the masks for each instance and save them as follows:
```python
cd data_evaluate_LLM/eval_metrics/colors/MaskDINO/demo
python demo.py [Config File] [Input Images Directory] [Output Images Directory] [Model Weights]
```
Where:
```
1) Config File: config of segmentation model
2) Input Images Directory: path to folder images need to evaluate
3) Output Images Directory: path to output of segmented mask
4) Model weight: path ot the model weight downloaded above
```
For instance:
```

python demo.py --config-file 'T2I_benchmark/codes/eval_metrics/colors/MaskDINO/configs/coco/instance-segmentation/swin/maskdino_R50_bs16_50ep_4s_dowsample1_2048.yaml' \
--input 'Attend-and-Excite/colors/*' \
--output Attend-and-Excite/ \
--opts MODEL.WEIGHTS T2I_benchmark/weights/mask_dino/maskdino_swinl_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask52.3ap_box59.pth
```

Then, run the 
[hue_based_color_classifier.py](colors/hue_based_color_classifier.py)
to calculate the color composition accuracy, as follows:
```python
cd data_evaluate_LLM/eval_metrics/colors/
python hue_based_color_classifier.py [Generated Masks Directory] [GT-csv] [T2I Model Output Directory]
```
Where:
```
1) Generated Masks Directory: the Output Images Directory in previous running
2) GT-csv: is the csv file which can be generated by running the prompt generation code or downloaded directly from our published prompts.
3) T2I Model Output Directory:  path to folder images need to evaluate
```
For instance:
```
python hue_based_color_classifier.py  'MaskDINO/demo/Attend-and-Excite' '../../HRS/colors_composition_prompts.csv' 'Attend-and-Excite/colors'
```
